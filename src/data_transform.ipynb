{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a891ed5f",
   "metadata": {},
   "source": [
    "#### Merge data by appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafeca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install pandas numpy\n",
    "%pip install --upgrade pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077be562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved merged data to: /home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data.csv\n",
      "Merged DataFrame (timestamp, appName, duration columns):\n",
      "       timestamp                        appName  duration\n",
      "0  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "1  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "2  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "3  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "4  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "\n",
      "Shape of the resulting DataFrame: (58650000, 3)\n",
      "Data saved to: /home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "# Path to the first CSV file (provided)\n",
    "error_file_path = '/home/tarhone/Manulife_Data_Predictive/data/transactionError_APM.csv'\n",
    "# Path to the second CSV file\n",
    "# IMPORTANT: Please ensure 'transaction_APM.csv' exists in your 'data' directory\n",
    "# or update the path accordingly.\n",
    "transaction_file_path = '/home/tarhone/Manulife_Data_Predictive/data/transaction_APM.csv'\n",
    "\n",
    "# Define output file path (same directory as source files)\n",
    "output_file_path = '/home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data.csv'\n",
    "\n",
    "try:\n",
    "    # Read the CSV files into pandas DataFrames\n",
    "    df_error = pd.read_csv(error_file_path)\n",
    "    df_transaction = pd.read_csv(transaction_file_path)\n",
    "\n",
    "    # Select the required columns ('appName', 'timestamp', 'duration') from the error data.\n",
    "    # The 'timestamp' and 'duration' values from this DataFrame will be in the final result.\n",
    "    df_error_selected = df_error[['appName', 'timestamp', 'duration']]\n",
    "\n",
    "    # Select only the 'appName' column from the transaction data for the join.\n",
    "    # This is used to find common appNames.\n",
    "    df_transaction_keys = df_transaction[['appName']]\n",
    "\n",
    "    # Merge the two DataFrames using an inner join on 'appName'.\n",
    "    # This keeps only rows where 'appName' exists in both DataFrames.\n",
    "    # The 'timestamp' and 'duration' columns will originate from df_error_selected.\n",
    "    merged_df = pd.merge(df_error_selected, df_transaction_keys, on='appName', how='inner')\n",
    "\n",
    "    # Keep all three columns: timestamp, appName, duration in the specified order\n",
    "    result_df = merged_df[['timestamp', 'appName', 'duration']]\n",
    "\n",
    "    # Save the merged data to CSV file\n",
    "    result_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Successfully saved merged data to: {output_file_path}\")\n",
    "\n",
    "    # Display the first few rows of the resulting DataFrame and its shape\n",
    "    print(\"Merged DataFrame (timestamp, appName, duration columns):\")\n",
    "    print(result_df.head())\n",
    "    print(f\"\\nShape of the resulting DataFrame: {result_df.shape}\")\n",
    "    print(f\"Data saved to: {output_file_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or both CSV files were not found. Please check the paths:\")\n",
    "    print(f\"  Path for error data: '{error_file_path}'\")\n",
    "    print(f\"  Path for transaction data: '{transaction_file_path}'\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: A required column was not found in one of the CSV files: {e}\")\n",
    "    print(\"Please ensure 'appName' column exists in both files, and 'timestamp', 'duration' columns exist in 'transactionError_APM.csv'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609121e7",
   "metadata": {},
   "source": [
    "#### Read data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba1319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总行数: 58650000\n",
      "列名: ['timestamp', 'appName', 'duration']\n",
      "--------------------------------------------------\n",
      "显示前100行:\n",
      "        timestamp                        appName  duration\n",
      "0   1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "1   1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "2   1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "3   1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "4   1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "..            ...                            ...       ...\n",
      "95  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "96  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "97  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "98  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "99  1747380636485  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "\n",
      "==================================================\n",
      "显示随机100行:\n",
      "              timestamp                        appName  duration\n",
      "42010879  1745682916359  pos-apply-sgp-aks-sea-emm-prd     0.758\n",
      "45093632  1745558272181  pos-apply-sgp-aks-sea-emm-prd     0.042\n",
      "25681655  1746497676635  pos-apply-sgp-aks-sea-emm-prd     0.023\n",
      "17949737  1746690838217  pos-apply-sgp-aks-sea-emm-prd     0.008\n",
      "9223791   1747139963815  pos-apply-sgp-aks-sea-emm-prd     0.042\n",
      "...                 ...                            ...       ...\n",
      "27168326  1746453747761  pos-apply-sgp-aks-sea-emm-prd     0.021\n",
      "1528129   1747369405029  pos-apply-sgp-aks-sea-emm-prd     0.024\n",
      "54524828  1745134680370  pos-apply-sgp-aks-sea-emm-prd     0.042\n",
      "28319043  1746453346214  pos-apply-sgp-aks-sea-emm-prd     0.027\n",
      "365528    1747379686677  pos-apply-sgp-aks-sea-emm-prd     0.033\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "\n",
      "==================================================\n",
      "数据基本统计信息:\n",
      "          timestamp      duration\n",
      "count  5.865000e+07  5.865000e+07\n",
      "mean   1.746257e+12  9.393043e-01\n",
      "std    7.377872e+08  4.191462e+00\n",
      "min    1.744810e+12  7.000000e-03\n",
      "25%    1.745564e+12  2.400000e-02\n",
      "50%    1.746433e+12  3.100000e-02\n",
      "75%    1.747027e+12  5.500000e-02\n",
      "max    1.747381e+12  3.007100e+01\n",
      "\n",
      "==================================================\n",
      "数据类型信息:\n",
      "timestamp      int64\n",
      "appName       object\n",
      "duration     float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Read and display the merged CSV file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the merged file just saved\n",
    "merged_file_path = '/home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data.csv'\n",
    "\n",
    "try:\n",
    "    # Read the merged CSV file\n",
    "    df_merged = pd.read_csv(merged_file_path)\n",
    "    \n",
    "    print(f\"Total rows: {len(df_merged)}\")\n",
    "    print(f\"Column names: {list(df_merged.columns)}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # If the number of rows is greater than or equal to 100, display the first 100 rows\n",
    "    if len(df_merged) >= 100:\n",
    "        print(\"Display first 100 rows:\")\n",
    "        print(df_merged.head(100))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Display 100 random rows:\")\n",
    "        # Set random seed for reproducibility\n",
    "        random_sample = df_merged.sample(n=100, random_state=42)\n",
    "        print(random_sample)\n",
    "        \n",
    "    else:\n",
    "        # If the number of rows is less than 100, display all rows\n",
    "        print(f\"There are only {len(df_merged)} rows in total, displaying all data:\")\n",
    "        print(df_merged)\n",
    "    \n",
    "    # Display basic statistical information\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Basic statistical information:\")\n",
    "    print(df_merged.describe())\n",
    "    \n",
    "    # Display data types\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Data type information:\")\n",
    "    print(df_merged.dtypes)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found {merged_file_path}\")\n",
    "    print(\"Please make sure to run the merge code above to generate the CSV file first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186664c2",
   "metadata": {},
   "source": [
    "#### Transform timestamp to datetime and keep in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a313a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据:\n",
      "数据形状: (58650000, 3)\n",
      "列名: ['timestamp', 'appName', 'duration']\n",
      "timestamp列的前5个值:\n",
      "0    1747380636485\n",
      "1    1747380636485\n",
      "2    1747380636485\n",
      "3    1747380636485\n",
      "4    1747380636485\n",
      "Name: timestamp, dtype: int64\n",
      "timestamp数据类型: int64\n",
      "--------------------------------------------------\n",
      "检测到13位时间戳（毫秒），将转换为秒...\n",
      "转换后的数据:\n",
      "数据形状: (58650000, 3)\n",
      "列名: ['timestamp', 'appName', 'duration']\n",
      "前10行数据:\n",
      "             timestamp                        appName  duration\n",
      "0  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "1  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "2  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "3  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "4  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "5  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "6  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "7  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "8  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "9  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
      "--------------------------------------------------\n",
      "已保存格式化后的数据到: /home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data_formatted.csv\n",
      "\n",
      "转换前后对比（前5行）:\n",
      "   original_timestamp  formatted_timestamp                        appName  \\\n",
      "0       1747380636485  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd   \n",
      "1       1747380636485  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd   \n",
      "2       1747380636485  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd   \n",
      "3       1747380636485  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd   \n",
      "4       1747380636485  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd   \n",
      "\n",
      "   duration  \n",
      "0      0.04  \n",
      "1      0.04  \n",
      "2      0.04  \n",
      "3      0.04  \n",
      "4      0.04  \n"
     ]
    }
   ],
   "source": [
    "# transform timestamp column to YYYY-MM-DD HH:MM:SS format\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# read file\n",
    "merged_file_path = '/home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data.csv'\n",
    "output_file_path = '/home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data_formatted.csv'\n",
    "\n",
    "try:\n",
    "    \n",
    "    df = pd.read_csv(merged_file_path)\n",
    "    \n",
    "    print(\"Original data:\")\n",
    "    print(f\"Data shape: {df.shape}\")\n",
    "    print(f\"Column names: {list(df.columns)}\")\n",
    "    print(\"First 5 values of the timestamp column:\")\n",
    "    print(df['timestamp'].head())\n",
    "    print(f\"Data type of timestamp column: {df['timestamp'].dtype}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check if the timestamp is a Unix timestamp (usually 10 or 13 digit number)\n",
    "    # If the timestamp column is already in datetime format, skip conversion\n",
    "    if pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        print(\"Detected timestamp column is already in datetime format, skipping conversion...\")\n",
    "        df['timestamp_formatted'] = df['timestamp']\n",
    "    else:\n",
    "        sample_timestamp = df['timestamp'].iloc[0]\n",
    "    sample_timestamp = df['timestamp'].iloc[0]\n",
    "    \n",
    "    if len(str(int(sample_timestamp))) == 13:\n",
    "        # 13-digit timestamp (milliseconds), will convert to seconds...\n",
    "        print(\"Detected 13-digit timestamp (milliseconds), will convert to seconds...\")\n",
    "        df['timestamp_formatted'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    elif len(str(int(sample_timestamp))) == 10:\n",
    "        # 10-digit timestamp (seconds)\n",
    "        print(\"Detected 10-digit timestamp (seconds)...\")\n",
    "        df['timestamp_formatted'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    else:\n",
    "        # Try direct conversion\n",
    "        print(\"Trying to directly convert timestamp...\")\n",
    "        df['timestamp_formatted'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    \n",
    "    # Format as YYYY-MM-DD HH:MM:SS string\n",
    "    df['timestamp_formatted'] = df['timestamp_formatted'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Create the final DataFrame, keeping all original columns and only replacing the timestamp column\n",
    "    df_final = df.copy()\n",
    "    df_final['timestamp'] = df['timestamp_formatted']\n",
    "    df_final.drop(columns=['timestamp_formatted'], inplace=True)\n",
    "    \n",
    "    print(\"Transformed data:\")\n",
    "    print(f\"Data shape: {df_final.shape}\")\n",
    "    print(f\"Column names: {list(df_final.columns)}\")\n",
    "    print(\"First 10 rows:\")\n",
    "    print(df_final.head(10))\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save the transformed data to a new CSV file\n",
    "    df_final.to_csv(output_file_path, index=False)\n",
    "    print(f\"Formatted data has been saved to: {output_file_path}\")\n",
    "    \n",
    "    # Show comparison before and after conversion (first 5 rows):\n",
    "    print(\"\\nComparison before and after conversion (first 5 rows):\")\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'original_timestamp': df['timestamp'].head(),\n",
    "        'formatted_timestamp': df['timestamp_formatted'].head(),\n",
    "        'appName': df['appName'].head(),\n",
    "        'duration': df['duration'].head()\n",
    "    })\n",
    "    print(comparison_df)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found {merged_file_path}\")\n",
    "    print(\"Please make sure to run the merge code above to generate the CSV file first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while converting timestamp format: {e}\")\n",
    "    print(\"Possible reasons:\")\n",
    "    print(\"1. The timestamp column contains non-numeric values.\")\n",
    "    print(\"2. The timestamp format is not a standard Unix timestamp.\")\n",
    "    print(\"3. There are missing or abnormal values in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a68169b",
   "metadata": {},
   "source": [
    "#### Print data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b07fb723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>appName</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-16 07:30:36</td>\n",
       "      <td>pos-apply-sgp-aks-sea-emm-prd</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-16 07:30:36</td>\n",
       "      <td>pos-apply-sgp-aks-sea-emm-prd</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-16 07:30:36</td>\n",
       "      <td>pos-apply-sgp-aks-sea-emm-prd</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-16 07:30:36</td>\n",
       "      <td>pos-apply-sgp-aks-sea-emm-prd</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-16 07:30:36</td>\n",
       "      <td>pos-apply-sgp-aks-sea-emm-prd</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                        appName  duration\n",
       "0  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
       "1  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
       "2  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
       "3  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04\n",
       "4  2025-05-16 07:30:36  pos-apply-sgp-aks-sea-emm-prd      0.04"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/tarhone/Manulife_Data_Predictive/data/merged_transaction_data_formatted.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffdfc6b",
   "metadata": {},
   "source": [
    "### Aggreate duration avg to minutes level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c60ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data time range:\n",
      "Earliest time: 2025-04-16 13:33:27\n",
      "Latest time: 2025-05-16 07:30:36\n",
      "Time span: 29 days 17:57:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412/3066705588.py:18: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  pd.Grouper(key='timestamp', freq='T'),  # 'T' represents minute-level grouping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after minute-level aggregation: (281, 3)\n",
      "Processed data shape: (42838, 3)\n",
      "Shortest series length: 42838\n",
      "Longest series length: 42838\n",
      "Average series length: 42838.0\n",
      "First 10 rows:\n",
      "            timestamp                        appName  duration\n",
      "0 2025-04-16 13:33:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "1 2025-04-16 13:34:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "2 2025-04-16 13:35:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "3 2025-04-16 13:36:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "4 2025-04-16 13:37:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "5 2025-04-16 13:38:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "6 2025-04-16 13:39:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "7 2025-04-16 13:40:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "8 2025-04-16 13:41:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "9 2025-04-16 13:42:00  pos-apply-sgp-aks-sea-emm-prd     0.058\n",
      "\n",
      "Number of series meeting minimum 1008 observations requirement: 1\n",
      "Ready for forecasting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1412/3066705588.py:36: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  complete_time_range = pd.date_range(start=start_time, end=end_time, freq='T')\n",
      "/tmp/ipykernel_1412/3066705588.py:47: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged['duration'] = merged['duration'].fillna(method='ffill').fillna(group_mean if not pd.isna(group_mean) else 0.1)\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing: Aggregate to minute level and create regular time series\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('../data/merged_transaction_data_formatted.csv')\n",
    "\n",
    "# Convert timestamp to datetime type\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "print(\"Original data time range:\")\n",
    "print(f\"Earliest time: {df['timestamp'].min()}\")\n",
    "print(f\"Latest time: {df['timestamp'].max()}\")\n",
    "print(f\"Time span: {df['timestamp'].max() - df['timestamp'].min()}\")\n",
    "\n",
    "# Aggregate data by minute - calculate average duration per minute\n",
    "df_minutely = df.groupby([\n",
    "    pd.Grouper(key='timestamp', freq='T'),  # 'T' represents minute-level grouping\n",
    "    'appName'\n",
    "]).agg({\n",
    "    'duration': 'mean'  \n",
    "}).reset_index()\n",
    "\n",
    "# Remove null values\n",
    "df_minutely = df_minutely.dropna()\n",
    "\n",
    "print(f\"Data shape after minute-level aggregation: {df_minutely.shape}\")\n",
    "\n",
    "# Create complete time series for each appName (fill missing minutes)\n",
    "def create_complete_time_series(group):\n",
    "    # Get time range\n",
    "    start_time = group['timestamp'].min()\n",
    "    end_time = group['timestamp'].max()\n",
    "    \n",
    "    # Create complete minute sequence\n",
    "    complete_time_range = pd.date_range(start=start_time, end=end_time, freq='T')\n",
    "    \n",
    "    # Create complete DataFrame\n",
    "    complete_df = pd.DataFrame({'timestamp': complete_time_range})\n",
    "    complete_df['appName'] = group['appName'].iloc[0]\n",
    "    \n",
    "    # Merge original data\n",
    "    merged = pd.merge(complete_df, group, on=['timestamp', 'appName'], how='left')\n",
    "    \n",
    "    # Fill missing values (using forward fill or group mean)\n",
    "    group_mean = group['duration'].mean()\n",
    "    merged['duration'] = merged['duration'].fillna(method='ffill').fillna(group_mean if not pd.isna(group_mean) else 0.1)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Process each appName\n",
    "df_processed = pd.concat([\n",
    "    create_complete_time_series(group) \n",
    "    for name, group in df_minutely.groupby('appName')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Sort data\n",
    "df_processed = df_processed.sort_values(['appName', 'timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(\"Processed data shape:\", df_processed.shape)\n",
    "\n",
    "# Check length of each series\n",
    "series_lengths = df_processed.groupby('appName').size()\n",
    "print(f\"Shortest series length: {series_lengths.min()}\")\n",
    "print(f\"Longest series length: {series_lengths.max()}\")\n",
    "print(f\"Average series length: {series_lengths.mean():.1f}\")\n",
    "\n",
    "print(\"First 10 rows:\")\n",
    "print(df_processed.head(10))\n",
    "\n",
    "# Save processed data\n",
    "df_processed.to_csv('../data/processed_minutely_data.csv', index=False)\n",
    "\n",
    "# Check if it meets the requirement of 1008 observations\n",
    "min_required = 1008\n",
    "satisfying_series = series_lengths[series_lengths >= min_required]\n",
    "print(f\"\\nNumber of series meeting minimum {min_required} observations requirement: {len(satisfying_series)}\")\n",
    "\n",
    "if len(satisfying_series) > 0:\n",
    "    print(\"Ready for forecasting!\")\n",
    "else:\n",
    "    print(\"Still need more observations or further data extension\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
